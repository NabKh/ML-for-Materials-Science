{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 5: Model Evaluation\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NabKh/ML-for-Materials-Science/blob/main/Tutorial-07-ML-Discovery/notebooks/05_model_evaluation.ipynb)\n\n> **Before You Start:** Please check the [INSTALLATION_GUIDE.md](../../INSTALLATION_GUIDE.md) for setup instructions. For Google Colab:\n> ```python\n> !pip install pymatgen matminer shap -q\n> ```\n> Then restart the runtime (Runtime â†’ Restart runtime).\n\n---\n\n## ğŸ¯ Learning Objectives\n\n1. **Use** proper metrics (MAE, RMSE, RÂ²) and understand when each matters\n2. **Implement** k-fold cross-validation for robust evaluation\n3. **Visualize** learning curves to diagnose overfitting/underfitting\n4. **Tune** hyperparameters with Optuna\n\n---\n\n**â±ï¸ Estimated time: 75 minutes** | **ğŸ“š Difficulty: ğŸŸ¡ Intermediate**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# Create figures directory\nos.makedirs('figures', exist_ok=True)\n\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, KFold, learning_curve\n)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ntry:\n    import optuna\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    HAS_OPTUNA = True\nexcept:\n    HAS_OPTUNA = False\n    print(\"Optuna not installed, hyperparameter tuning section will use grid search\")\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-darkgrid')\nnp.random.seed(42)\nprint(\"âœ… Ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 1. Understanding Evaluation Metrics\n\n### ğŸ“– Theory\n\n<div style=\"background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%); padding: 20px; border-radius: 10px; border-left: 4px solid #6366f1;\">\n\n**Why proper evaluation matters:**\n- Training error is always optimistic (model has \"seen\" the data)\n- We need to estimate how well the model generalizes to **unseen data**\n- Different metrics capture different aspects of error\n\n</div>\n\n### Regression Metrics\n\n**Mean Absolute Error (MAE):**\n$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n\n- Measures average magnitude of errors\n- Same units as the target variable\n- **Robust to outliers** (linear penalty)\n\n**Root Mean Squared Error (RMSE):**\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n\n- Penalizes large errors more heavily (quadratic penalty)\n- Same units as target variable\n- **Sensitive to outliers**\n\n**Coefficient of Determination (RÂ²):**\n$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n\n- Proportion of variance explained by the model\n- Ranges from $-\\infty$ to 1 (1 = perfect, 0 = mean baseline)\n- **Unitless** - good for comparing models\n\n### When to Use Each Metric\n\n| Metric | Best For | Limitation |\n|--------|----------|------------|\n| MAE | Interpretable errors, outlier-robust | Doesn't penalize large errors |\n| RMSE | When large errors are especially bad | Sensitive to outliers |\n| RÂ² | Comparing models, variance explained | Can be misleading for non-linear patterns |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "X = np.random.randn(n, 10)\n",
    "y = 2*X[:, 0] + 1.5*X[:, 1]**2 + 0.5*np.random.randn(n)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_s, y_train)\n",
    "y_pred = rf.predict(X_test_s)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "print(f\"  RÂ²:   {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 2. Cross-Validation\n\n### ğŸ“– Theory\n\n<div style=\"background: rgba(14, 165, 233, 0.1); padding: 15px; border-radius: 10px; border-left: 4px solid #0ea5e9;\">\n\n**The Problem with a Single Train-Test Split:**\n- Results depend heavily on which samples end up in train vs test\n- Single estimate has high variance\n- May get \"lucky\" or \"unlucky\" with the split\n\n**Solution: K-Fold Cross-Validation**\n\n1. Split data into $k$ equal folds\n2. For each fold $i$: train on folds $\\{1,...,k\\} \\setminus i$, test on fold $i$\n3. Average the $k$ test scores\n\n</div>\n\n### Mathematical Framework\n\nThe cross-validated score estimates the expected prediction error:\n\n$$\\text{CV}(k) = \\frac{1}{k}\\sum_{i=1}^{k}\\text{Error}(\\text{fold}_i)$$\n\n**Variance of CV estimate decreases with more folds:**\n$$\\text{Var}(\\text{CV}) \\approx \\frac{\\sigma^2}{k}$$\n\nBut more folds = more computation and potential for correlated training sets.\n\n**Common choices:**\n- **k=5**: Good balance (80% train, 20% test each fold)\n- **k=10**: Lower variance, more computation\n- **LOOCV (k=n)**: Unbiased but high variance, expensive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 5-fold cross-validation with professional visualization\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\n\ncv_scores = cross_val_score(rf, X, y, cv=5, scoring='r2')\n\nprint(\"5-Fold Cross-Validation Results:\")\nprint(f\"  Scores: {cv_scores.round(4)}\")\nprint(f\"  Mean:   {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n\n# Professional visualization\nfig, ax = plt.subplots(figsize=(10, 5), facecolor='white')\n\ncolors = {\n    'bars': '#6366f1',\n    'mean': '#ef4444',\n    'text': '#1e293b',\n    'grid': '#e2e8f0'\n}\n\n# Create bars\nbars = ax.bar(range(1, 6), cv_scores, color=colors['bars'], alpha=0.8, \n              edgecolor='white', linewidth=2)\n\n# Add value labels on bars\nfor bar, score in zip(bars, cv_scores):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n            f'{score:.3f}', ha='center', va='bottom', fontsize=11, \n            fontweight='bold', color=colors['text'])\n\n# Mean and std band\nax.axhline(cv_scores.mean(), color=colors['mean'], linestyle='--', linewidth=2, \n           label=f'Mean: {cv_scores.mean():.3f}')\nax.fill_between([0.5, 5.5], cv_scores.mean()-cv_scores.std(), cv_scores.mean()+cv_scores.std(), \n                alpha=0.15, color=colors['mean'], label=f'Â±1 std: {cv_scores.std():.3f}')\n\nax.set_xlabel('Fold', fontsize=12, color=colors['text'])\nax.set_ylabel('RÂ² Score', fontsize=12, color=colors['text'])\nax.set_title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold', color=colors['text'])\nax.set_xticks(range(1, 6))\nax.set_ylim(0, 1.05)\nax.set_xlim(0.4, 5.6)\nax.legend(loc='lower right', fontsize=10, framealpha=0.95)\nax.set_facecolor('white')\nax.grid(True, axis='y', alpha=0.3, color=colors['grid'])\n\n# Add interpretation\nax.text(0.02, 0.02, f'Low variance (Â±{cv_scores.std():.3f}) indicates stable model performance', \n        transform=ax.transAxes, fontsize=10, color=colors['text'], style='italic',\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='#f1f5f9', edgecolor=colors['grid']))\n\nplt.tight_layout()\nplt.savefig('figures/05_cross_validation.png', dpi=200, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\\nFigure saved to figures/05_cross_validation.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 3. Learning Curves: Diagnosing Model Behavior\n\n### ğŸ“– Theory\n\n<div style=\"background: rgba(16, 185, 129, 0.1); padding: 15px; border-radius: 10px; border-left: 4px solid #10b981;\">\n\n**Learning curves** show how model performance changes with training set size.\n\nThey help diagnose the **bias-variance tradeoff**:\n\n$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n\n</div>\n\n### Interpreting Learning Curves\n\n| Pattern | Training Score | Validation Score | Diagnosis | Solution |\n|---------|---------------|------------------|-----------|----------|\n| **Underfitting** | Low | Low | High bias | More complex model |\n| **Overfitting** | High | Low (big gap) | High variance | More data, regularization |\n| **Good fit** | High | High (converging) | Balanced | Keep current approach |\n\n### Visual Guide\n\n```\n     Underfitting          Overfitting           Good Fit\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n RÂ² â”‚â”â”â” Train   â”‚   RÂ² â”‚â”â”â”â”â”â”â” Tr  â”‚   RÂ² â”‚â”â”â”â”â”â”â” Tr  â”‚\n    â”‚â”â”â” Valid   â”‚      â”‚            â”‚      â”‚â”â”â”â”â”â” Val  â”‚\n    â”‚            â”‚      â”‚â”â”â” Valid   â”‚      â”‚            â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       Training Size       Training Size       Training Size\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate and visualize learning curves\ntrain_sizes, train_scores, test_scores = learning_curve(\n    RandomForestRegressor(n_estimators=50, random_state=42),\n    X, y, cv=5, n_jobs=-1,\n    train_sizes=np.linspace(0.1, 1.0, 10),\n    scoring='r2'\n)\n\n# Professional plot\nfig, ax = plt.subplots(figsize=(11, 6), facecolor='white')\n\ncolors = {\n    'train': '#0ea5e9',      # Sky blue\n    'valid': '#ec4899',      # Pink\n    'text': '#1e293b',\n    'grid': '#e2e8f0'\n}\n\ntrain_mean = train_scores.mean(axis=1)\ntrain_std = train_scores.std(axis=1)\ntest_mean = test_scores.mean(axis=1)\ntest_std = test_scores.std(axis=1)\n\n# Plot with confidence bands\nax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n                alpha=0.2, color=colors['train'])\nax.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, \n                alpha=0.2, color=colors['valid'])\nax.plot(train_sizes, train_mean, 'o-', color=colors['train'], linewidth=2.5, \n        markersize=8, label='Training score')\nax.plot(train_sizes, test_mean, 's-', color=colors['valid'], linewidth=2.5, \n        markersize=8, label='Validation score')\n\nax.set_xlabel('Training Set Size', fontsize=12, color=colors['text'])\nax.set_ylabel('RÂ² Score', fontsize=12, color=colors['text'])\nax.set_title('Learning Curve: Random Forest', fontsize=14, fontweight='bold', color=colors['text'])\nax.legend(loc='lower right', fontsize=11, framealpha=0.95)\nax.set_facecolor('white')\nax.grid(True, alpha=0.3, color=colors['grid'])\nax.set_ylim(0.5, 1.02)\n\n# Add gap annotation\nfinal_gap = train_mean[-1] - test_mean[-1]\nax.annotate('', xy=(train_sizes[-1]+5, test_mean[-1]), \n            xytext=(train_sizes[-1]+5, train_mean[-1]),\n            arrowprops=dict(arrowstyle='<->', color='#f59e0b', lw=2))\nax.text(train_sizes[-1]+15, (train_mean[-1] + test_mean[-1])/2, \n        f'Gap: {final_gap:.2f}', fontsize=10, color='#f59e0b', fontweight='bold', va='center')\n\n# Add interpretation box\ninterpretation = \"Converging curves = Good fit\" if final_gap < 0.1 else \"Large gap = Overfitting risk\"\nbox_color = '#10b981' if final_gap < 0.1 else '#f59e0b'\nax.text(0.02, 0.98, interpretation, transform=ax.transAxes, fontsize=11, \n        color='white', fontweight='bold', va='top',\n        bbox=dict(boxstyle='round,pad=0.4', facecolor=box_color, edgecolor='none'))\n\nplt.tight_layout()\nplt.savefig('figures/05_learning_curve.png', dpi=200, bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\\nğŸ’¡ Interpretation:\")\nprint(f\"  - Final training score: {train_mean[-1]:.3f}\")\nprint(f\"  - Final validation score: {test_mean[-1]:.3f}\")\nprint(f\"  - Gap: {final_gap:.3f}\")\nif final_gap < 0.1:\n    print(\"  âœ… Curves are converging - good model fit!\")\nelse:\n    print(\"  âš ï¸ Large gap suggests overfitting - consider regularization or more data\")\n    \nprint(\"\\nFigure saved to figures/05_learning_curve.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 4. Hyperparameter Tuning\n\n### ğŸ“– Theory\n\n<div style=\"background: rgba(245, 158, 11, 0.1); padding: 15px; border-radius: 10px; border-left: 4px solid #f59e0b;\">\n\n**Hyperparameters** are model settings that are NOT learned from data:\n- Random Forest: `n_estimators`, `max_depth`, `min_samples_split`\n- Neural Networks: learning rate, hidden layer sizes, dropout\n\n**Goal:** Find hyperparameters that minimize validation error.\n\n</div>\n\n### Tuning Strategies\n\n| Method | How it Works | Pros | Cons |\n|--------|--------------|------|------|\n| **Grid Search** | Try all combinations | Simple, thorough | Expensive, curse of dimensionality |\n| **Random Search** | Random combinations | More efficient | May miss optimal |\n| **Bayesian (Optuna)** | Learn from past trials | Most efficient | More complex |\n\n### Optuna's Approach\n\nOptuna uses **Tree-structured Parzen Estimator (TPE)** to model:\n\n$$P(\\text{hyperparams} | \\text{good score})$$\n\nIt samples hyperparameters more likely to give good results based on previous trials."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_OPTUNA:\n",
    "    def objective(trial):\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=3, scoring='r2')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\nâœ… Best RÂ²: {study.best_value:.4f}\")\n",
    "    print(f\"Best parameters: {study.best_params}\")\n",
    "else:\n",
    "    print(\"Optuna not available. Install with: pip install optuna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Exercises\n",
    "\n",
    "### Exercise: Compare Different CV Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare 3-fold, 5-fold, and 10-fold CV\n",
    "# Which gives the lowest variance in scores?\n",
    "\n",
    "# for k in [3, 5, 10]:\n",
    "#     scores = cross_val_score(rf, X, y, cv=k, scoring='r2')\n",
    "#     print(f\"{k}-fold: {scores.mean():.4f} Â± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>---\n\n## âœ… Module Summary\n\n### Key Takeaways\n\n<div style=\"background: rgba(99, 102, 241, 0.1); padding: 15px; border-radius: 10px; border-left: 4px solid #6366f1;\">\n\n| Concept | Key Formula | When to Use |\n|---------|-------------|-------------|\n| **MAE** | $\\frac{1}{n}\\sum\\|y - \\hat{y}\\|$ | Interpretable errors, robust to outliers |\n| **RMSE** | $\\sqrt{\\frac{1}{n}\\sum(y - \\hat{y})^2}$ | When large errors are especially costly |\n| **RÂ²** | $1 - \\frac{SS_{res}}{SS_{tot}}$ | Comparing models on same dataset |\n| **K-Fold CV** | Average over $k$ test folds | Robust performance estimation |\n| **Learning Curves** | Score vs. training size | Diagnose over/underfitting |\n\n</div>\n\n### Decision Flowchart\n\n```\nEvaluate Model\n    â”‚\n    â”œâ”€â–º Calculate metrics (MAE, RMSE, RÂ²)\n    â”‚\n    â”œâ”€â–º Run k-fold cross-validation\n    â”‚       â””â”€â–º Check variance across folds\n    â”‚\n    â”œâ”€â–º Plot learning curves\n    â”‚       â”œâ”€â–º High train, low valid â†’ Overfitting\n    â”‚       â”œâ”€â–º Both low â†’ Underfitting  \n    â”‚       â””â”€â–º Converging â†’ Good fit\n    â”‚\n    â””â”€â–º Tune hyperparameters with Optuna\n            â””â”€â–º Use CV score as objective\n```\n\n### Common Pitfalls to Avoid\n\n1. **Data leakage**: Always split BEFORE any preprocessing\n2. **Over-tuning**: Use separate test set for final evaluation\n3. **Ignoring variance**: Report CV mean Â± std, not just mean\n4. **Wrong metric**: Choose metric based on business/scientific needs\n\n### What's Next?\n\nIn **Module 6: Explainable AI**, you'll learn to:\n- Interpret model predictions with SHAP values\n- Understand feature contributions\n- Build trust in ML predictions\n\n---\n\n**ğŸ“š Continue to Module 6:** [Explainable AI](06_explainable_ai.ipynb)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}